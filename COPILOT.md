# COPILOT.md - AI agent instructions

This file defines the interaction protocol, coding standards, and workflow for AI agents collaborating on this 'agentception' project.

## Project overview

**Agentception** is a self-hosted, offline AI orchestrator designed to run locally using FOSS models via Ollama. It bridges the gap between a raw LLM and a functional system agent by providing a "Think-Act-Observe" loop. The agent can read/write files, execute shell commands, and perform assistant tasks (like web searching and note-taking) within a sandboxed Docker environment.

For full technical details, see `Specification.md`.

## Core principles

- **All code generated by AI agent** (no copy-paste from external sources without attribution)
- **Human review & approval** required before moving to the next phase
- **Token efficiency**: use mini models (0.33x multiplier) for most work; one premium request available
- **Transparent prompting**: all interactions logged in `SESSION.md`
- **Clean separation**: instructions in `COPILOT.md`, session history in `SESSION.md`

## Code style guidelines

### Naming conventions

- **variables/functions**: `snake_case` (standard Python PEP 8).
- **classes**: `PascalCase`.
- **constants**: `UPPER_SNAKE_CASE`.
- **tool naming**: Use descriptive names for LLM tools (e.g., `execute_shell_command` instead of `run_cmd`).

### Project-specific standards

- **asynchronous code**: use `asyncio` for the orchestrator loop and FastAPI bridge to ensure non-blocking UI updates.
- **type hinting**: mandatory type hints for all function signatures.
- **path safety**: always use `pathlib.Path` for file operations to ensure cross-platform compatibility within and outside the Docker container.
- **error handling**: Every tool execution must be wrapped in a try-except block that returns a string error message to the LLM rather than crashing the orchestrator.

## Testing guidelines

- **unit tests**: create tests in `tests/` for core tool logic (e.g., verifying `write_file` doesn't escape the workspace).
- **mocking**: mock the Ollama API responses to test the orchestrator's decision-making logic without consuming local compute.
- **integration**: test the full "loop" by providing a known prompt and verifying the expected file system changes in the sandbox.

## Git & commit guidelines

Follow the Conventional Commits specification. After completing each step or fixing a bug, suggest a commit with:

### Commit types
- `feat`: New feature (e.g., `feat: add glass material refraction`)
- `fix`: Bug fix (e.g., `fix: resolve sphere intersection overflow`)
- `docs`: Documentation updates
- `refactor`: Code changes (no bug fix or feature)
- `style`: Formatting, linting
- `test`: Unit test additions or updates
- `chore`: Build, dependency, or config changes

### Commit message format
```
<type>(<scope>): <subject>

<body>

*commit co-authored-by Copilot as part of a 'vibe-coding' educational project*
```

### Commit discipline
- commit after each working feature (not broken intermediate states)
- one logical change per commit
- clear, concise messages describing the "what" and "why"
- if a commit fixes an issue, reference it (e.g., "Fixes #5")

## Development workflow

1. **receive prompt** from human with step-by-step instructions
2. **implement feature** following all code style and modularity guidelines
3. **test** following all testing guidelines
4. **create clean commit(s)** with conventional message
5. **log prompt in `SESSION.md`** following Documentation & session logging guideline
5. **wait for human review** before proceeding to next step

## Documentation & session logging

All interactions must be logged in `SESSION.md` in the following format:

```
[timestamp] - [feature/task description]

**Prompt:**
[The exact prompt sent to the model]

**Model:** [Model name, e.g., GPT-4o, Claude 3.5 Sonnet, Haiku]
```

## Resources

- **specification**: `Specification.md` — complete technical requirements
- **README**: `README.md` — project overview and context
- **tests**: `tests/` — unit tests (create as needed)